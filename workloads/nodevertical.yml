---
#
# Runs NodeVertical on an existing RHCOS cluster
#

- name: Runs NodeVertical on a RHCOS cluster
  hosts: orchestration
  gather_facts: true
  remote_user: "{{orchestration_user}}"
  vars_files:
    - vars/nodevertical.yml
  vars:
    workload_job: "nodevertical"
  tasks:
    - name: Create scale-ci-tooling directory
      file:
        path: "{{ansible_user_dir}}/scale-ci-tooling"
        state: directory

    - name: Copy workload files
      copy:
        src: "{{item.src}}"
        dest: "{{item.dest}}"
      with_items:
        - src: scale-ci-tooling-ns.yml
          dest: "{{ansible_user_dir}}/scale-ci-tooling/scale-ci-tooling-ns.yml"

    - name: Slurp kubeconfig file
      slurp:
        src: "{{kubeconfig_file}}"
      register: kubeconfig_file_slurp

    - name: Slurp ssh private key file
      slurp:
        src: "{{pbench_ssh_private_key_file}}"
      register: pbench_ssh_private_key_file_slurp

    - name: Slurp ssh public key file
      slurp:
        src: "{{pbench_ssh_public_key_file}}"
      register: pbench_ssh_public_key_file_slurp

    - name: Remove nodevertical label from nodes
      shell: |
        oc label nodes -l nodevertical=true nodevertical-

    - name: Label worker nodes with pbench agent for nodevertical
      shell: |
        oc get nodes -l pbench_agent=true,node-role.kubernetes.io/worker= | grep worker | head -n 2 | awk '{print $1}' | xargs -I % oc label nodes % nodevertical=true
      when: enable_pbench_agents

    - name: Label worker nodes without pbench agent for nodevertical
      shell: |
        oc get nodes -l node-role.kubernetes.io/worker= --show-labels | grep worker | grep -v "pbench_agent=true" | head -n 2 | awk '{print $1}' | xargs -I % oc label nodes % nodevertical=true
      when: enable_pbench_agents

    - name: Label worker nodes when there are no pbench agents
      shell: |
        oc get nodes | grep worker | head -n 4 | awk '{print $1}' | xargs -I % oc label nodes % nodevertical=true
      when: not enable_pbench_agents

    - name: Calculate maximum pods to fit in nodevertical labeled space
      shell: |
        for node in $(oc get nodes -l="nodevertical=true" | awk 'NR > 1 {print $1}'); do
          pods_running=$(oc describe node $node | grep -w "Non-terminated \Pods:" | awk  '{print $3}' | sed "s/(//g")
          pod_count=$(( pod_count+pods_running ))
        done
        echo "$(( {{nodevertical_maxpods}}-pod_count ))"
      register: total_pod_count

    - name: Template workload templates
      template:
        src: "{{item.src}}"
        dest: "{{item.dest}}"
      with_items:
        - src: pbench-cm.yml.j2
          dest: "{{ansible_user_dir}}/scale-ci-tooling/pbench-cm.yml"
        - src: pbench-ssh-secret.yml.j2
          dest: "{{ansible_user_dir}}/scale-ci-tooling/pbench-ssh-secret.yml"
        - src: kubeconfig-secret.yml.j2
          dest: "{{ansible_user_dir}}/scale-ci-tooling/kubeconfig-secret.yml"
        - src: workload-job.yml.j2
          dest: "{{ansible_user_dir}}/scale-ci-tooling/workload-job.yml"
        - src: workload-nodevertical-script-cm.yml.j2
          dest: "{{ansible_user_dir}}/scale-ci-tooling/workload-nodevertical-script-cm.yml"

    - name: Check if scale-ci-tooling namespace exists
      shell: |
        oc get projects | grep "scale-ci-tooling"
      ignore_errors: true
      changed_when: false
      register: scale_ci_tooling_ns_exists

    # This delete should block until complete
    - name: Block for existing tooling namespace
      block:
        - name: Ensure any stale scale-ci-workload job is deleted
          shell: |
            oc delete job scale-ci-workload -n scale-ci-tooling
          ignore_errors: true
      when:
        - scale_ci_tooling_ns_exists.rc == 0

    - name: Block for non-existing tooling namespace
      block:
        - name: Create tooling namespace
          shell: |
            oc create -f {{ansible_user_dir}}/scale-ci-tooling/scale-ci-tooling-ns.yml

        - name: Create tooling service account
          shell: |
            oc create serviceaccount useroot -n scale-ci-tooling
            oc adm policy add-scc-to-user privileged -z useroot -n scale-ci-tooling
          when: enable_pbench_agents
      when:
        - scale_ci_tooling_ns_exists.rc != 0

    # Create Secrets for workload job
    - name: Create/replace kubeconfig secret
      shell: |
        oc replace --force -n scale-ci-tooling -f "{{ansible_user_dir}}/scale-ci-tooling/kubeconfig-secret.yml"

    - name: Create/replace the pbench configmap
      shell: |
        oc replace --force -n scale-ci-tooling -f "{{ansible_user_dir}}/scale-ci-tooling/pbench-cm.yml"

    - name: Create/replace pbench ssh secret
      shell: |
        oc replace --force -n scale-ci-tooling -f "{{ansible_user_dir}}/scale-ci-tooling/pbench-ssh-secret.yml"

    - name: Create/replace workload script configmap
      shell: |
        oc replace --force -n scale-ci-tooling -f "{{ansible_user_dir}}/scale-ci-tooling/workload-nodevertical-script-cm.yml"

    - name: Create/replace workload job to that runs workload script
      shell: |
        oc replace --force -n scale-ci-tooling -f "{{ansible_user_dir}}/scale-ci-tooling/workload-job.yml"

    # Might need retires to account for api server cert rotation / connection breaking
    # Wait for job start, then wait for complete
    # - name: Wait for job to complete
    #   shell: |
    #     oc logs -f job/scale-ci-nodevertical -n scale-ci-tooling
